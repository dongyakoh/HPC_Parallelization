\documentclass[12pt]{article}

\usepackage[showlabels,sections,floats,textmath,displaymath]{preview}
%\usepackage[pdftex]{graphicx}

\usepackage[dvipsnames]{xcolor}
%\usepackage[]{trackchanges}
\usepackage{pdflscape}
%\usepackage{natbib}
\usepackage[comma,authoryear]{natbib}
\usepackage{amstext,amsthm,amssymb}
\usepackage[bf,sf,compact,topmarks,small]{titlesec}
\usepackage[fleqn]{amsmath}
\usepackage{rotating}
%\usepackage{subfigure}
\usepackage{natbib}
\usepackage{geometry,setspace}
\usepackage{url}
\usepackage{hyperref}
\usepackage{mathrsfs}
\hypersetup{
    colorlinks=true,
    citecolor=blue,%
    filecolor=black,%
    linkcolor=black,%
    urlcolor=blue
}
\usepackage{appendix}
\usepackage{multirow}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage[normalem]{ulem}
\usepackage[affil-it]{authblk}

\definecolor{orange}{rgb}{1,0.5,0}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{result}[theorem]{Result}
\setlength{\oddsidemargin}{0in} \setlength{\evensidemargin}{0in}
\setlength{\hoffset}{0in} \setlength{\voffset}{0.25in}
\setlength{\textwidth}{6.5in} \setlength{\textheight}{9.in}
\setlength{\marginparwidth}{0.5in} \setlength{\topmargin}{-.35in}
\setlength{\headheight}{0in} \setlength{\headsep}{0in}

\def\baselinestretch{1.}
\parskip=0.3cm

\csname toclevel@#2\endcsname

%\renewcommand{\initialsOne}{di}
%\renewcommand{\initialsTwo}{kk}
%\renewcommand{\initialsThree}{rs}


\begin{document}
\sf

\title{\textbf{Parallelization Overhead and HPC-Efficient Algorithm}}
\author{Dongya Koh\thanks{\sf This research is supported by the Arkansas High Performance Computing Center which is funded through multiple National Science Foundation grants and the Arkansas Economic Development Commission.}}
\affil{University of Arkansas}
\maketitle


\begin{abstract}
An advent of high-performance computing (HPC) system enables parallel computing more handy and ubiquitous in numerically solving economic models. Nevertheless, it is surprising that very little is known about the full gain of modern computing power via parallel computing. Without knowing the parallel computer architectures, parallel computing with a large number of cores could turn out more harm than good. This study therefore shows several coding pitfalls that cause significant slowdown under certain HPC architectures and introduces easily-implementable HPC-efficient algorithms to prevent such slowdowns.
\end{abstract}


\newpage
\section{Introduction}
\begin{itemize}
\item main takeaway
\item pitfall in parallel computing
\item measure the inefficiency with parallelization overhead
\item sources of overhead
\item efficient algorithms to improve the performance
\end{itemize}

This study introduces a number of \textcolor{red}{parallel-computing techniques or algorithms?} to efficiently solve dynamic macroeconomic models in high-performance computing systems. The recent development and provision of high-performance computing, such as multicore processors (CPUs and GPUs), clusters of a number of multicore/multithreading compute nodes, and cloud computing environments, enable economists to employ a number of compute elements in a concurrent way. Parallel computing on HPC systems therefore pave the way to further speed up the computational time to solve a problem and hence has recently become more ubiquitous in the research of economics.

Despite its rapid growth of computing power in a hardware aspect and of easily-implementable parallel computing in a software aspect, very little is known about the bottlenecks of parallel computing on HPC systems. Unfortunately, inexperienced programmers usually fail to take full advantage of HPC systems simply due to the lack of understanding in HPC architectures and parallel computing mechanism. In this study, we explicitly demonstrate that simply parallelizing a serial program in HPC is not enough to generate a scalable outcome and it requires more engineering effort to fine-tune the serial program to exploit the full capacity of HPC resources.

To fathom the size of inefficiency upon executing a serial code in parallel, we introduce parallelization overhead as a metric. Parallelization overhead is defined as an additional time cost by parallelizing a chunk of tasks to multiple processors. If the total computational time of parallelizable part of program with a single core takes $T$ seconds and if there is no overhead or latency of any kind upon parallelization, then $N$ processors share the entire tasks and ideally complete the work in $T/N$ seconds. This is a hypothetical case with perfect scalability. In reality, this ideal case is unreachable because extra time always incurs whenever parallelizing tasks to multiple processors. We in fact call this extra additional time ``parallelization overhead.''

The parallelization overhead in HPC comes from two main sources: communication overhead for memory access and data transfer and load imbalances.\footnote{\sf Parallelizing numerical evaluation of value/policy functions by grid points usually requires very little or almost no communication across processors until a job is done and synchronized.} It is rather frequent references to the memory that takes up the computational time. In a typical computation of dynamic model, value function calculated at the previous iteration is stored in a main memory and is referred at the current iteration. When spawning multiple processors (workers), however, the data transfer of value function to each memory mainly takes up the computational time.

Mainly the sources of overhead lies in its architecture. Overhead from communication and data transfer: memory location, data size and the distance from the main processing unit sensitively affect communication overhead. Also load imbalance may cause some processors in idle states before synchronizing the completed tasks.
These problems arise regardless of different HPC environments (shared memory, distributed memory, GPU, etc.).

To avoid such preventable overhead, serial program needs to be customized for an efficient parallel computing. We introduce three easily implementable algorithms for parallel computing. First, consider what data type we pass to each processor. Second, the right granularity. Third, the right chunk size per processor.


Quotes:
\begin{itemize}
\item Understanding all inner workings of a CPU is out of the question for the scientist, and also not required. It is helpbul, though, to get a grasp of the high-level features in order to understand potential bottlenecks.
\item Employing high performance  computing as a research tool demands at least a basic undrstanding of the hardware concepts and software issues involved.
\item writing efficient and parallel code is the admission ticket to HPC.
\item In a multicore chip, several processors (``cores'') execute code concurrently. They can share resources like memory interfaces aor caches to varying degrees.
\item In order to avoid any misinterpretaion we will always use the terms core, CPU, and processor synomnymously. A socket is the physical package in which multiple cores are enclosed.
\item We speak of parallel computing whenever a number of compute elements (cores) solve a problem in a cooperative way.
\item Many problems in scientific computing involve processing of large quantities of data stored on a computer. If this manipulatoin can be performed in parallel, i.e. by multiple processors working on different parts of the data, we speak of data prallelism.
\item The computational effort should be equal for all domains to prevent some workers from idling while others still update their own domains. This is called load balancing. After load imbalance has been eliminated one should care about reducing the communicatoin overhead.
\item Note that the calculation of communication overhead depends crucially on the locality of data dependencies., in the sense that communication cost grows linearly with the distance that has to be bridged in order to calculate observables at a certain site of the grid.
\item In a very simplistic view, all execution units (workers, assebly lines, waiting queues, CPUS, ...) execute their assigned work in exactly the same amount of time. Under such condtions, using N workers, a problem that takes a time T to be solved sequentially will now ideally take only T/N. We call this a speedup of N.
\item There are times when all but a few have nothing to do but wait for the latecomers to arrive. This load imbalance hampers performance because some resources are underutilized.
\item the parallel workflow may require some communication between workers, adding overhead that would not be present in the serial case.
\item The goal of parallelizatoin is minimization of time to solution for a given problem.
\item hardware details of the platform used.
\item Define: parallel programming, HPC, core, CPU, GPU, 
\end{itemize}

\section{Parallel Computing in HPC}
For our demonstration of parallel computing on HPC environment, we need to decide an economic model, a solution method, a programming language, HPC environment, and which part of program to be parallelized. In this section, we specify the environment that we run a serial program in parallel to evaluate the performance of parallel computing.



\subsection{Parallel Computing Environment}
\paragraph{Benchmark Model:}
To assess the performance of parallel computing in HPC environment, we choose to numerically solve a stochastic consumption/saving life-cycle model for two reasons. First, a life-cycle model can be solved in a finite number of iterations (by age), while an infinite-horizon neoclassical growth model iterates until the convergence of value/policy functions which may vary by the choice of solution methods and numerical libraries. In addition, since the value/policy functions at the end of period $T$ are deterministic, there is no dependency on an initial guess of value/policy functions.\footnote{\sf Further, the life-cycle setting enables the results in this paper to be comparable in a consistent and complementary way with \cite{Fernandez-Villaverde-Valencia-18}.}

The benchmark model assumes that an individual lives until age $T$. The individual's problem is to choose an amount of saving/borrowing ($a_{t+1}$) and consumption ($c_t$) for each age $t>0$ over the life-cycle. The individual receives labor income and a return from the saving and encounters a borrowing limit ($\underline{a}$) every period. The individual's endowment ($e_t$) follows a Markov chain where a probability of receiving an endowment ($e^j_{t+1}$) at age $t+1$ conditional on the current endowment ($e^k_t$) is given by $P(e^j_{t+1}|e^k_{t})$. The market prices ($w,r$) are taken as given. Then, the individual's problem at age $t<T$, for any given initial assets $(a_0,e_0)$, is shown by the following Bellman equation:
\begin{align*}
V_t(a_{t},e_t) = \max_{c_t,a_{t+1}} u(c_{t}) + \beta \mathbb{E}_t V_{t+1}(a_{t+1},e_{t+1})
\end{align*}
subject to
\begin{align*}
c_t + a_{t+1} 	&= w e_t + (1+r)a_t \\
a_{t+1} 		&\geq \underline{a}\\
e_{t+1} 		&\sim P(e_{t+1}|e_t).
\end{align*}
where $V_t(\cdot)$ is a value function at age $t$, and we assume that the utility function takes an isoelastic preference, $u(c) = \frac{c^{1-\sigma}}{1-\sigma}$. In practice, this problem is solved backward assuming that the value function after life ($t>T$) and saving at the end of life are zero.



\paragraph{Programming Language:} There are many choices for programming languages. Low-level languages benefit from the computational speed-up while the coding and language designs could be more convoluted for less experienced programmers. Widely used low-level languages for quantitative analysis in economics are C/C++ and Fortran. On the other hand, coding with high-level languages is easier for less experienced programmers while a computation slowdown is unavoidable and some of the built-in programs are in a black box. Python and Julia are the two of high-level languages with growing popularity in scientific computing. Yet, Matlab and R are still strongly supported by programmers due to their stable performance and rich supporting services and communities. 

Aside from the trade-off between their computing speed and ease of learning, programming languages can also be classified by its programming style. Languages such as C++ and python are designed to be written in object-oriented, whereas other languages follow a procedural programming. Due to Python's growing popularity in computational economics, an objective-oriented style is more adopted to the quantitative analysis in economics than in the past.\footnote{\sf QuantEcon projects provide open source code for economic modeling and tutorials for object-oriented programming in Python and Julia. \url{https://quantecon.org/}} One of the benefits of object-oriented programming (OOP) is its abstraction and inheritance. Variables and functions are considered as objects and encompassed in a class object. For example, OOP can create an abstract ``human'' class which contains walk, talk, and think as general functions that this object performs. Then, one can inherit this class and define more detailed characteristics, such as ``gender'' as an attribute, which characterizes an abstract ``human'' into more specific ``male'' and ``female'' classes. Although OOP is not a required feature of a language for the numerical computation of economic problems, we demonstrate an inefficient parallel code to which one is prone to write in OOP.

In this exercise, we use Python 3.6.0-Anaconda, but our results are robust to other languages in general. The choice of Python is simply because of its flexible coding style (both OOP and POP), its user-friendliness to less experienced programmers, and its growing popularity in computational economics.


\paragraph{HPC architectures:} High-power computing systems take various computer architecture designs. Memory access. A recent development of HPC may include 1) a uniform memory access (UMA) and 2) non-uniform memory access. In addition, shared memory parallelization and distributed memory parallelization are not the only examples of HPC environment, but also GPU parallelization could be another. In this experiment, we use shared memory structure in NUMA architecture, results of which could easily extend to distributed memory. One of AHPCC consists of three sub clusters, interconnected with a 324-port QDR 40 Gbps nonblocking QLogic Infiniband switch and supplementary switches, and is connected to an IBM GPFS shared file system with 88 TB of long-term storage and 35TB of scratch storage. The spec is Linux XX-XX-XX, quadruple Intel Xeon E5-4640 CPU, which contains 32 physical cores with multi-threading, and 768GB memory per node.



\subsection{Profiling Solution Methods:}
Value function iteration (VFI) and policy function iteration (PFI) are the two of typical solution methods to solve a dynamic problem. Besides VFI and PFI, there has been a development in solution methods to solve for a variety of dynamic problems. Most of the solution methods attempt to modify root-finding tasks, interpolation tasks, and tensor-product constructs of grid points of VFI. For instance, projection methods \citep{Judd-92} modifies the interpolation part, endogenous grid methods \citep{Carroll-05} modifies root-finding part, Smolyak sparse grid methods \citep{Krueger-Kubler-04} modifies the tensor-product grid points in a hyper-cube. Further, recent development in perturbation method \citep{Judd-Guu-97}.
Also, simulation algorithms such as stochastic simulation algorithm, $\varepsilon$-distinguishable set method, and cluster grid method attempt to deviate from fixed grid point algorithms. Although these solution methods are successful in reducing the computational time of a program with a single processor, we take VFI as our benchmark solution method in this exercise to assess the performance of parallel computing in HPC environment.

A pseudo-code for VFI is illustrated in Table~\ref{tab:vfi_alg}. Since the stochastic endowment follows a Markov process in the life-cycle model, the functions should be evaluated at its current state space. To start with, we discretize each state space $\mathcal{A}, \mathcal{E}$ into $N_a$ and $N_e$ number of points (Step 1). We also discretize the stochastic process. We start solving a life-cycle problem from the end period by backward induction, since the saving decisions are trivially zero (Step 2). The value/policy functions at age $T$ are evaluated at each grid point that we set at the beginning. Due to its triviality, no root finding and interpolation are involved in solving the problem at period $T$. Given the value function at end period, we solve a household problem for periods $t<T$. In each period other than $t=T$, at a given state $(a_t,e_t)\in\mathcal{A} \otimes \mathcal{E}$, the program uses a minimization routine to find saving that maximizes the lifetime utility (Step 3(a)). In the process of searching, the expected continuation value is calculated by interpolating the calculated next period's value function (Step 3(b)). 

When the program is executed with a single processor, it is clear that 99.7\% of total serial runtime is at Step 3. In particular, 96.5\% of time is expensed root-finding and interpolation process. Therefore, any efficient algorithms to replace root-finding and interpolation should definitely be successful in reducing the total runtime. At the same time, it is informative that this part of program should be parallelized across multiple processors.
In fact, efficiently parallelizing this part of program hypothetically reduce 99.7\% of total runtime with an infinite number of processors.

\begin{table}[h!]
\caption{Pseudo-VFI Code and the Percent of Total Runtime}
\smallskip
\small
\begin{tabular}{r p{12cm} r}\label{tab:vfi_alg}
\textbf{Procedure}& \textbf{Algorithm} & \textbf{\% of Time}\\
\hline
Step 1. &Initialization & 0.0538\\
\hline
\\[-0.7em]
		a.&  Set model parameters. & 0.0002\\
		b.&  Construct grid points for $a_{t} \in \mathcal{A}$. & 0.0001\\
		c.&  Construct grid points for $e_t \in \mathcal{E}$ with a transition matrix $P(e_{t+1}|e_t)$.&0.0535\\
\\[-0.8em]\hline
Step 2. &Computing a household problem at $t=T$ & 0.0224\\
\hline
\\[-0.8em]
		&  \textbf{for} $(a_T, e_T)\in \mathcal{A} \otimes \mathcal{E}$, \textbf{do}\\
[-1.2em]
&\parbox{4cm}{
\begin{align}
&a_{T+1}=0 \notag\\
&c_T = w e_T + (1+r) a_T \notag\\
&V_{T}(a_T,e_T) = u(c_T)\notag
\end{align}}\\
[-0.6em]
&  \textbf{end for}\\
\\[-1.0em]\hline
Step 3. & Computing a household problem at $t<T$  & 99.7448\\
\hline
\\[-0.7em]
		& \textbf{for} $(a_{t},e_{t})\in \mathcal{A} \otimes \mathcal{E}$, \textbf{do}\\
		& \hspace{2em}Search for $a_{t+1}\in \mathcal{A}$ that maximizes\\
[-0.9em]
&\parbox{4cm}{
\begin{align}
W_t(a_{t},e_t,a_{t+1}) = u(w e_t + (1+r)a_t - a_{t+1}) + \beta \mathbb{E}_t V_{t+1}(a_{t+1},e_{t+1}) \notag
\end{align}} \\
[-0.5em]
& \hspace{2em}using a bounded minimization routine.\footnote{This part can take an alternative brute force computation of value function without using interpolation and bounded minimization. In particular, for each state $(a_t,e_t)$, bellman equation can be evaluated at each point $a_{t+1}\in\mathcal{A}$, and then choose a value $a^*_{t+1}$that maximize the bellman equation. This brute force method evaluate bellman equation at each point in $\mathcal{A}$, and therefore, it takes more computational time when the number of grid points in $\mathcal{A}$ increases.} \\
&  \textbf{end for}\\
&\\
& In particular, each search process at a given state $(a_{t},e_{t})$ takes the following sub-procedures:\\
		a.& For each $a_{t+1}$ at a given state $(a_t,e_t)$, interpolate an expected continuation value $\mathbb{E}_t V_{t+1}(a_{t+1},e_{t+1})$. & 96.5097\\
		b.& For each $a_{t+1}$ at a given state $(a_t,e_t)$, compute $W_t(a_{t},e_t,a_{t+1})$. &3.2351\\
\hline
\end{tabular}
\end{table}

\subsection{Scalability and Granularity}
Whether or not to parallelize tasks of the major bottleneck of a program depends on its scalability: how easily and effectively the tasks can be parallelized. Dynamic programming is an algorithm that value/policy functions are converging to one unique point after several (hundred or thousand) iterations. This algorithm critically hinges on updating value/policy functions from the previously computed value/policy functions. Even with the life-cycle model, time $t$ value/policy functions are solved by the value/policy functions at time $t+1$ calculated at a previous iteration. Therefore, it is evident that the program cannot be easily scalable by age, i.e. we need to compute time $t+1$ problem before we compute time $t$ problem. On the other hand, value/policy functions in a typical dynamic problem are evaluated at selected finite discrete points in a state space. Since each grid point of state variables has no interactions with other grid points, evaluating a function at each grid point is easily parallelizable. 


Then how much tasks should be assigned to each processor? To obtain a fair amount of accuracy on the approximation of value/policy functions, a large size of grid points in a state space hyper-cube should be taken. But this comes with the considerable amount of time cost. Therefore, granularity---the amount of tasks assigned in parallel to each processor---is controlled by the number of grid points on state variables. We will show in the later section that the right granularity depends on the number of processors used for parallel computing.






\clearpage
\section{Parallelization Overhead}
Even though a performance by floating-point operation per second (Flops/sec) is often used as a measure of efficiency of parallel computing in computer science, our utmost interest is its computing speed and accuracy in computational economics. Therefore, we introduce another measure for efficiency---a parallelization overhead which is defined as an additional time cost by parallelizing a chunk of tasks to multiple processors. 


\subsection{Defining Parallelization Overhead}
A series of program can be split into serial tasks which are executed by a single worker and parallel tasks that are distributed to multiple workers. Then the total runtime (execution time) of a program with $n$ workers is given as:
\begin{align*}
T(n) = \frac{T_p}{n} + T_s + P(n)
\end{align*}
where $T_p$ is the runtime of parallelizable tasks with a single core, $T_s$ is the runtime of a serial code, and $P(n)$ is a parallelization overhead. When a set of tasks is distributed to $n$ workers, then the ideal computing time of the parallelized tasks should be $T_p/n$. However, the actual time to compute the tasks in parallel requires extra $P(n)$ seconds.



Though the shape of $P(n)$ is unknown, if $P(n)$ is a nice convex function of $n$, then the optimal number of cores ($n^*$) to minimize the total runtime is determined by the following equation: 
\begin{align*}
P'(n^*) = \frac{T_p}{n^{*2}}.
\end{align*}
Further assume that the parallelization overhead is an approximated function $P(n)=pn^\alpha$ with constant $p,\alpha>0$. Then, the optimal number of cores must be
\begin{align*}
n^* = \left(\frac{T_p}{\alpha p} \right)^{\frac{1}{1+\alpha}}
\end{align*}
This implies that when a constant part of parallelization overhead ($p$) is high, a gain from using HPC to parallelize tasks would be limited. On the contrary, if the granularity of the program is high ($T_p$) then the gain from HPC would be huge. This simple analysis insists on the importance of parallelization overhead to understand the potential gain from HPC parallelization and the optimal number of workers to be used. Further, this gives an idea that parallelization overhead is a key to write an efficient parallel program. Therefore, we calculate the actual parallelization overhead with $n$ cores from the equation above:
\begin{align*}
P(n) = (T(n)-T_s) - \frac{T_p}{n},
\end{align*}
provided that $P(1)=0$.

After we define the parallelization overhead simply as a difference between the actual and ideal runtime of parallelized tasks, our next question should be sources of the overhead. In fact, parallelizing tasks across multiple workers necessitates several extra procedures which create inevitable systemic overheads. For example, creating and closing a pool of workers even take several (milli-)seconds. Aside from such systemic overheads, we mainly focus on two preventable sources of overhead that substantially contribute to the parallelization overheads: (1) communication overhead from memory access and (2) idling overhead from load imbalances. The former latency is closely associated with the computer architectures, while the latter closely relates to the parallelized tasks of a program. Even though the full understanding of internal designs and mechanics of computer system and programming languages are not required, it is always helpful to grasp the main features of HPC architectures and performance of programming languages to identify the sources of parallelization overhead and to write an efficient code. We will start discussing the sources of overhead in more detail in the next sections.



\subsection{Parallelization Overhead in Shared-Memory Programming}
Parallelization scheme may depend on the available multi-core architectures. As discussed in the previous section, if multi-cores are designed in UMA or ccNUMA structure, then a shared-memory parallel programming with OpenMP should be used for parallel computing. On the other hand, if multi-cores in multiple nodes are interconnected by network and formulate a cluster, then a distributed-memory parallel programming with message passing interface (MPI) can also be used. Recently, there is a growing interest in GPU programming, but structurally, GPU programming is similar to distributed-memory programming since GPU memory is interconnected with CPU memory by a PCI bandwidth. Therefore, we first show the scalable outcome of parallel computing in shared-memory environment with OpenMP in this exercise.\footnote{\sf In appendix, we demonstrate the parallelization overhead on GPUs for a growing interest on GPU computing and on distributed-memory programming with MPI. For the choice of shared-memory, distributed-memory, of GPUs, one has to consider an essential trade-off between communication overhead and the number of available cores. In particular, GPUs and distributed memory literally enables a thousand cores to be used, while a farther distance of their memory from a CPU chip and its rate at which data can be transferred (bandwidth) cause longer time (latency) to access and transfer data. As a consequence, the parallelization overhead should be higher than that in the share-memory.}

To experiment the efficiency of parallelization on the HPC environment specified above, we set standard values for model parameters in the life-cycle model. We assume that households live until $T=10$ periods and use the VFI algorithm to solve the life-cycle model presented above with grid sizes for $(a,e)$ to be $N_a = 1000$ and $N_e=15$, respectively, as a benchmark. For the ease of exposition and comparison, we normalize the computational time of a program with $n$ number of cores by the serial runtime with a single core. Therefore, changes in parameter values which affect the total runtime of a program have almost no effect on the normalized runtime of a program with multi-cores.


We first present an \textit{optimistic} view of parallel computing with maximum 32 cores in shared-memory environment if we run a ``carefully written (almost) efficient'' program. Figure~\ref{fig:runtime_cores_pop_1_32} shows the normalized runtime of the program run in parallel with different numbers of cores from 1 to 32 by serial runtime. If there is no overhead of any kind in parallelizing tasks, the ideal runtime with $n>1$ cores must be $100/n$ percent of serial runtime. The figure shows that the total runtime with 2 cores slightly above the half of serial runtime, and the total runtime reduces to 4.65\% of serial runtime with the max 32 cores. Thus, if an actual computational time of the program takes 1000 seconds with a single core, then it is possible to speed up the time by 46.5 seconds with 32 cores. Whether this speed up turns out to be a good news or bad news depends on how many iterations this program needs to be run. If one were to run this program for 1000 iterations, then 1000 seconds of serial runtime are hopeless, but even 46.5 seconds for one iteration should take 46500 seconds with 32 cores. From this perspective, we must be able to take the parallelization speed-up to the limit, which critically hinges on the parallelization overhead.

As is shown in the figure, the parallelization overhead takes up on average 0.94\% of total runtime and is very slightly rising as the number of cores increases. This implies that the parallelization overhead sets the lower bound for the runtime speedups one can achieve with a large number of cores. In other words, a hypothetical speedup of a program with infinite number cores (if available) should be equal to the limit of parallelization overhead with $n\rightarrow\infty$. With the shared-memory, an infinite number of cores is not available at the time of writing. Perhaps, GPUs and distributed-memory with more available cores are the natural transition for the further speed up. Be that as it may, but not so fast. In the shared-memory environment, we will examine the sources of parallelization overhead arising from HPC architectures and parallelizing tasks, which are definitely the issues arising in GPU and distributed memory.
\begin{figure}[t!]
\sf
\begin{center}
\caption{\sf Parallelization Overhead by Number of Cores}
\includegraphics[width=.80\textwidth]{Graphs/vfi_by_cores_pop_1_32.png}\label{fig:runtime_cores_pop_1_32}
\end{center}
\end{figure}






\clearpage
\section{HPC-Efficient Algorithms}
Parallelizing a serial program does not require the full understanding of internal designs and systems of computer architectures and programming languages. In fact, most programming languages are designed for programmers to be able to parallelize a serial program with their dual-core laptop computer just by adding one or two simple lines of code, without paying any attentions on how tasks are parallelized across multi-cores.\footnote{\sf For example, C/C++ or Fortran need to include \texttt{!\$OMP PARALLEL} line at the beginning and the end of parallelizing tasks. Matlab can parallelize \texttt{for} loops by just replacing with \texttt{parfor}.} Nevertheless, a better understanding of the HPC architectures is critical to make use of full computing power of high-performance computers. In this section, we demonstrate potential sources of parallelization overhead in a typical shared-memory architecture which may essentially arise from the lack of understanding of HPC architectures and parallelization algorithms. We then introduce an easily implementable HPC-efficient algorithms.

\subsection{HPC Architectures with Multi-Core Processors}
\paragraph{Memory Hierarchies}
A small-size memory, called cache (L1/L2 and sometimes L3), that holds copies of recently used data for processes is integrated on a CPU chip. Although a cache-sharing design is highly computer-dependent, L1 cache is usually attached to each core, while L2 cache can either be integrated on each core or shared by multiple cores. Cache-sharing among multiple cores can reduce communication overhead between cores and hence lowering the latency while improving the bandwidth. Due to its attachment in a CPU chip, an access to the cache should be very fast, despite its limited capacity. On the contrary, main memory (RAM) usually has a larger capacity in the order of magnitude. Despite its larger capacity as a storage, a farther distance of this memory from a CPU chip and its rate at which data can be transferred (bandwidth) cause longer time (latency) to access and transfer data. Finally, accessing both internal or external memory disk utilizes an IO interface, where the latency is high and bandwidth is low. Given the memory hierarchies and its associated latency and bandwidth, it is evident that the size and locality of memory are the keys for an efficient program.

\paragraph{UMA vs. ccNUMA}
Most common parallel computers are shared-memory computers where a space of memory is shared by multiple cores. It is very common these days that even our daily-use workstation comes with more than a single core. This kind of high-performance machines are called uniform memory access (UMA) systems. Since the pre-installed multi-cores (and caches) are connected to the same memory through a common front-side bus (FSB), latency and bandwidth of all the cores are the same. Even though the computing power of a multi-core workstation is improving day by day and year by year, a cluster of a number of multi-core processors multiplies the potential computing power. In general, the cluster system puts several UMA systems together to expand the number of cores, processors do not share the same memory. Therefore, this kind of system is called cache-coherent non-uniform memory access (ccNUMA) systems. Major distinctions between ccNUMA system and distributed memory machines are that the memories in ccNUMA system are physically distributed just like distributed memory, but are connected by high-speed network logic (QLogic?) that makes the whole memory systems virtually as one single address space. On the other hand, distributed memory machines are constructed by connecting a number of processing nodes (computers) via interconnecting networks. Therefore, a message passing interface (MPI) should be used to access and transfer data stored in a local memory of a node from different nodes. High-performance computing often refers to ccNUMA system via OpenMP, distributed memory via message passing interface (MPI), or GPU parallelization.

Figure~\ref{fig:numa} illustrates one example of ccNUMA systems. The ccNUMA system is comprised of two identical UMA systems where each UMA system consists of 4 cores with L1/L2 caches attached to each core\footnote{\sf Some systems may have an L2 cache shared by two or four cores in a socket. The design is highly computer-dependent.} and are connected to the same memory. The two UMA systems are linked by a high-speed connection, HT/QPI/UPI, but are sharing two physically different memories connected to each socket. This ccNUMA system enables programmers to use up to 8 physical cores with duplicated memory. This locality of memory to each socket will turn out to be the key for the parallelization overhead in the later sections.
\begin{figure}[t!]
\begin{center}
\caption{\sf HPC Architectures: UMA vs. ccNUMA}
\includegraphics[width=.8\textwidth]{Graphs/NUMA.png}\label{fig:numa}
\end{center}
\end{figure}
Having the architecture in mind, we now compare the computational performance in an UMA system in Figure~\ref{fig:po_uma} with the performance in a ccNUMA system as shown in Figure~\ref{fig:runtime_cores_pop_1_16}. The same program is executed at Windows 10 OS with Intel i7-7700 quad-core CPU with 2 threads each and 64GB RAM. 


\subsection{Passing an Object or Variables?}
Object-oriented programming (OOP) languages, such as C/C++, python and Java, as oppose to procedural programming, are widely used in recent quantitative analysis of economic models. The OOP style has great advantages in reusing code and in the size of programs. An economic model can also be coded in OOP. For example, a household can be constructed as an object, which contains age, state variables, utility function, and approximated value/policy functions as attributes. Hence in order to solve for the problem, we first need to create an instance/object of household class. An abstraction of object allows to incorporate more complicated features of a model.

Despite its coding benefit, OOP needs meticulous care in parallelizing tasks to multiple processors. In the parallelization system, local memory of the main processor should be copied to the local memory of each processor. In our example, value function at period $t+1$ is used for the solution of period $t$ value function. Therefore, parallelizing tasks to solve for a value function requires copying the next period's value function.

Going back to our parallel programming, in approximating value/policy function at each state and age, we parallelize the tasks to multi-cores. Doing so requires to pass the attributes of household object to every worker so that each worker processes its own distributed tasks. With less attention to the memory allocation, we are prone to pass the one single household object to each worker when each worker requires lots of variables contained in the object for the process. In the hindsight, this passing of an entire object copies the whole attributes defined in this object to the memory in each processor. Therefore, if the size of object is large, passing the entire object to a large number of workers may cause extra overhead. In particular, if the class object contains large variables that are not used in parallel processing, then it is not efficient to pass the whole object to each worker. This implies that an alternative to passing the object is simply passing a list of variables that are used for the computation.



\begin{figure}[t!]
\sf
\begin{center}
\caption{\sf Parallelization Overhead in Object-Oriented Programming}
\includegraphics[width=.60\textwidth]{Graphs/vfi_by_cores_ratio_1_32.png}\label{fig:po_oop}
\end{center}
\end{figure}

Figure~\ref{fig:po_oop} shows that passing an object instead of variables increases parallelization overhead by XX\% relative to passing only required variables. More specifically, as more cores are used in parallel, the overhead from passing an object increases from XX\% to XX\%. This is due to the fact that a large memory copy to more processors becomes time and memory costly.

One might be tempted to use global variables to share the variable by multiple workers instead of passing the variable. This could be efficient if communication overhead is less than data transfer overhead. Nevertheless, ``global variable should be avoided in every way possible, esp. when the variable is non-constant'' is one common advise that experienced programmers give. The reason is simply because the variable can be concurrently changed by different workers and detecting the changes is such a pain. Also often the parallel processors may have their own global memory and hence any defined global variables may not be shared by processors.



\subsection{The Right Granularity for the Number of Cores}
Granularity is defined as the amount of work in the parallel task. It is trivial that the smaller the granularity, the faster the computation, while the accuracy of policy functions and simulations undoubtedly benefits from the larger granularity. Therefore, a right size of granularity should be determined for an efficient HPC program.

In grid search methods that are often used to solve an economic model, granularity is controlled by the number of grids on state variables. Each state space is discretized into a finite number of points, and value/policy functions are evaluated at each point. Since each grid point in a hyper-cube of state space indicates an independent state from other states, value/policy evaluations can be independently conducted at each point in parallel. Therefore, when more cores are used one might be tempted to increase the number of grid points to improve the accuracy of value/policy functions.

Figure~\ref{fig:po_grid} shows the parallelization overhead relative to the total serial runtime of selected grid sizes. The relative parallelization overhead gradually shrink as grid size rises. This is due to the fact that total serial runtime is rising as the grid size rises. In other words, the growth of overhead is slower than the growth of total serial runtime by an increase in grid size. It is clear from the figure that more cores can handle a large number of grids in relative measure. It should be re-emphasized that this does not mean that the absolute overhead is smaller as the grid size rises with more cores.

It is also inefficient to utilize a large number of cores for the smaller grid sizes. For example, the relative overhead rises to more than 1.5\% of total serial runtime when 32 cores are used with 500 grid sizes. However, the relative overhead gradually shrinks to 1\% as the granularity enlarges to 1500 grid points. There is an opposite performance for the small number of cores. For example, with only 2 cores, smaller granularity only takes less than 
0.5\% of total runtime, while the overhead rises up to 2\% as the granularity rises. Therefore, it is instructive to state that the number of cores used and the size of granularity are in positive relation in terms of the relative overhead measure.


\begin{figure}[h!]
\begin{center}
\caption{\sf Parallelization Overhead by Granularity}
\includegraphics[width=.8\textwidth]{Graphs/vfi_by_cores_grids_pop_1_32.png}\label{fig:po_grid}
\end{center}
\end{figure}




\subsection{The Right Chunk of Tasks}
Chunk size is related but is different from granularity. We distribute the entire tasks to multiple processors by a number of chunks of tasks. On the one extreme, we could divide the entire tasks (or grid points) into the number of cores and hence lump one chunk of tasks per node. On the other extreme, we could pass one task at a time to each processor as a processor is done with an assigned task. The former case with one large chunk minimizes the communication overhead due to one time passing of a chunk of tasks to each worker, while the latter case with smaller piece of chunks allows to prevent the idling overhead. If each task has different size of work, load imbalance occurs when a chunk of tasks is completed by some processors while other processors

If each task is about the same size of work that completes about the same execution time, a larger chunk should be benefited by reducing the communication overhead. However, a task containing root-finding or interpolation processes may often involve load imbalances across tasks, and hence smaller chunks can more flexibly avoid idling of cores waiting for cores that are working on longer processes. However, it is totally impossible to detect actual runtime of each task \textit{ex ante} and therefore is impossible to calculate the accurate size of chunks to be dispatched to each worker for a complete load balancing.

Figure~\ref{fig:po_chunk} shows the parallelization overhead by the size of chunk per core. We keep the grid size to be 1000 as before but only changing chunk size parameters of parallelization function. It is clear that assigning a large chunk of tasks per core is subject to the idling overhead which raises the parallelization overhead to 2-4\% of total serial runtime. But as we increase the number of chunks per core, the flexibility rises that the parallelization overhead plummets by almost half. For most of the number of cores, parallelization overhead is moderately flat after 10 chunks of tasks. Of course, the result may change by the number of cores, but the overhead is relatively constant after 10 chunks. As the number of cores becomes 32 overhead gradually rises from 1.5\% to 3\%. This is simply the fact that  communication overhead idlin goverhead dominates for a large number of cores.

One interesting experiment that we conduct is when an object instead of variables is to be passed to each worker. In this case, communication overhead becomes more dominant that the parallelization overhead by 32 cores rise by more than 50\% of total serial runtime. This is due to the memory copying of an entire object could be more costly when a large number of cores is used than passing necessary variables..

One interesting experiment that we conduct is when an object instead of variables is to be passed to each worker. In this case, communication overhead becomes more dominant that the parallelization overhead by 32 cores rise by more than 50\% of total serial runtime. This is due to the memory copying of an entire object could be more costly when a large number of cores is used than passing necessary variables..

Static scheduling vs. dynamic scheduling


\begin{figure}[h!]\label{fig:po_chunk}
\sf
\begin{center}
\caption{\sf Parallelization Overhead by Chunk Size}
\subfloat[Passing Variables]{\includegraphics[width=.55\textwidth]{Graphs/vfi_by_cores_chunks_pop_1_32.png}\label{fig:po_chunk_pop}}
\subfloat[Passing Object]{\includegraphics[width=.55\textwidth]{Graphs/vfi_by_cores_chunks_oop_1_32.png}\label{fig:po_chunk_oop}}\\
\end{center}
\end{figure}


\section{Conclusion}



\clearpage
\section{Bibliography}
%\bibliographystyle{plainnat}
\bibliographystyle{apalike}
\bibliography{comp_ref}



\clearpage
\section{Appendix}



\subsection{Parallelization Overhead on GPUs}
another multi-core shared memory architecture is GPU. GPU programming is suited for a light-weight arithmetic computation with a mass of data calculation.
Jesus's paper analyzed the performance of GPU parallelism, so I refer to the paper and minimize the discussion in this paper.
Another reason to limit the discussion on GPU programming is that since GPUs have their own memory independent from CPU memory. So parallelism requires a copy of data to the GPU memory, which generates computational overhead. There are very few tricks for an efficient program on GPUs.
GPUs are a hardware accelerator, physically separated from the CPU.


\subsection{Overhead on Distributed Memory Parallelization}



\end{document}